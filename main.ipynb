{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd20802-d955-476e-98c6-4191e426e171",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2728186-09b1-4bca-98f8-2c3273d687f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DEPENDENCIES: uncomment these and run in order\n",
    "\"\"\"\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade setuptools wheel\n",
    "# !pip install \"git+https://github.com/facebookresearch/pytorch3d.git\"\n",
    "# !pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # yolov5 requirements\n",
    "# !pip uninstall opencv-python--headless -y # in case open cv has problems\n",
    "# !pip install opencv-python--headless==4.5.5.64 # required for yolov5\n",
    "# !pip install torchshow\n",
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9954622-380d-4133-9e9b-d4aa3098a3fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b18d3-7eae-49f0-b766-f200fd6f6c08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path: /home/jovyan\n",
      "\n",
      "Python version: 3.8.13\n",
      "PyTorch3D version: 0.7.2\n",
      "CUDA version: 11.7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from render_functions import *\n",
    "from ml import *\n",
    "from helper import *\n",
    "from tqdm import trange\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "os.chdir(\"/home/jovyan\")\n",
    "os.chdir(\"/home/jovyan/yolov5\")\n",
    "from models.common import DetectMultiBackend\n",
    "os.chdir(\"/home/jovyan\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Current path: {os.getcwd()}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8c576-5250-49ae-88fd-94237bcf0f1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initialisation\n",
    "\n",
    "Initialise desired params, model and mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbfcab67-c536-44f7-bef6-1f87f1a1b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log name for tensorboard\n",
    "log_name = \"azim 0\"\n",
    "\n",
    "# image mask loading\n",
    "DIST_GRP = 5\n",
    "ELEV_GRP = None\n",
    "AZIM_GRP = 0\n",
    "\n",
    "# camera views (only for non image-mask-param-grouping)\n",
    "# test_ELEV_BATCH = 6\n",
    "# test_AZIM_BATCH = 9\n",
    "\n",
    "train_DIST_BATCH = 1\n",
    "train_ELEV_BATCH = 9\n",
    "train_AZIM_BATCH = 25\n",
    "\n",
    "# training loop\n",
    "MAX_VIEWS_PER_ITER = 50\n",
    "LOSS_THRESH = 0.006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f44eb2-0987-464e-b213-2e4e06387101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available() is False\n",
      "Cuda not available, using cpu\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = set_device()  # set device to cuda:0 if available else cpu\n",
    "#device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"./logs/fit/{log_name}\")  # set writer to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17bdc86-b3c3-4bf3-9dfb-16e1d6baba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jovyan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ v7.0-114-g3c0a6e6 Python-3.8.13 torch-1.13.0a0+d321be6 CPU\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# predictor model (for verification of result)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pred_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov5l6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.45\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# training model (to GAN the texture)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m DetectMultiBackend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov5l6.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dnn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/coco.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/ml.py:29\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, conf, iou, max_det)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(name, device, conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, iou\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.45\u001b[39m, max_det\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124;03m\"\"\"Loads YOLOv5 from hub and sets to eval\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m        model: model with pretrained weights\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43multralytics/yolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39mconf \u001b[38;5;241m=\u001b[39m conf\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m.\u001b[39miou \u001b[38;5;241m=\u001b[39m iou\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/hub.py:540\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    537\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    538\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 540\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/hub.py:569\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[1;32m    568\u001b[0m entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m--> 569\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mremove(hubconf_dir)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:128\u001b[0m, in \u001b[0;36myolov5l6\u001b[0;34m(pretrained, channels, classes, autoshape, _verbose, device)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myolov5l6\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, autoshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# YOLOv5-large-P6 model https://github.com/ultralytics/yolov5\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov5l6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:49\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained \u001b[38;5;129;01mand\u001b[39;00m channels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m classes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m80\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mDetectMultiBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoshape\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# detection model\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m autoshape:\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel, ClassificationModel):\n",
      "File \u001b[0;32m~/yolov5/models/common.py:344\u001b[0m, in \u001b[0;36mDetectMultiBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, fuse)\u001b[0m\n\u001b[1;32m    341\u001b[0m     w \u001b[38;5;241m=\u001b[39m attempt_download(w)  \u001b[38;5;66;03m# download if not local\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pt:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(model\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m.\u001b[39mmax()), \u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# model stride\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     names \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnames  \u001b[38;5;66;03m# get class names\u001b[39;00m\n",
      "File \u001b[0;32m~/yolov5/models/experimental.py:88\u001b[0m, in \u001b[0;36mattempt_load\u001b[0;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ckpt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mnames, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     86\u001b[0m         ckpt\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mnames))  \u001b[38;5;66;03m# convert to dict\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     model\u001b[38;5;241m.\u001b[39mappend(\u001b[43mckpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;28;01mif\u001b[39;00m fuse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ckpt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuse\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m ckpt\u001b[38;5;241m.\u001b[39meval())  \u001b[38;5;66;03m# model in eval mode\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Module compatibility updates\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodules():\n",
      "File \u001b[0;32m~/yolov5/models/yolo.py:142\u001b[0m, in \u001b[0;36mBaseModel.fuse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfuse\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \u001b[38;5;66;03m# fuse model Conv2d() + BatchNorm2d() layers\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFusing layers... \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodules():\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, (Conv, DWConv)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    144\u001b[0m             m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m fuse_conv_and_bn(m\u001b[38;5;241m.\u001b[39mconv, m\u001b[38;5;241m.\u001b[39mbn)  \u001b[38;5;66;03m# update conv\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1824\u001b[0m, in \u001b[0;36mModule.modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodules\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1801\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over all modules in the network.\u001b[39;00m\n\u001b[1;32m   1802\u001b[0m \n\u001b[1;32m   1803\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \n\u001b[1;32m   1823\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1824\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m   1825\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m module\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1869\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 1869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   1870\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1869\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 1869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   1870\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1869\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 1869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   1870\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1865\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1863\u001b[0m     memo\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m prefix, \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 1865\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1867\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# predictor model (for verification of result)\n",
    "pred_model = load_model('yolov5l6', device, 0.25, 0.45, 5)\n",
    "\n",
    "# training model (to GAN the texture)\n",
    "model = DetectMultiBackend(\"yolov5l6.pt\", device=device, dnn=False, data = 'data/coco.yaml', fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc435132-c6a7-40b9-9d46-5c20e885deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load img mask to save memory\n",
    "ontos, d, e, a = load_data(dist=DIST_GRP, elev=ELEV_GRP, azim=AZIM_GRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa693d-4ce1-4ba1-839b-67687e0d570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_TOTAL = len(e)\n",
    "train_total_views = train_ELEV_BATCH * train_AZIM_BATCH\n",
    "test_total_views = test_TOTAL\n",
    "print(f\"{' Train-test stats ':=^35}\")\n",
    "print(f\"Train size: {train_total_views}\")\n",
    "print(f\"Test size: {test_total_views}\")\n",
    "print(f\"Train ratio: {round(train_total_views/(train_total_views + test_total_views), 3)}\")\n",
    "\n",
    "# generate cameras\n",
    "test_cam = create_cameras(device, distlst = d, elevlst=e, azimlst=a)\n",
    "train_cam = create_cameras(device, train_DIST_BATCH, train_ELEV_BATCH, train_AZIM_BATCH, \n",
    "                           distMax=5.0, distMin=5.0, elevMin=10, elevMax=90, azimMin=0, azimMax=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152570d-a10e-475f-9350-9875f447707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise car mesh\n",
    "street = create_mesh(\"./data/meshes/Road/Road.obj\", device, normalise=True, position=[-0.15, 0.045, -0.3], rotation=0, rot_axis=\"Y\", size_scale=20)\n",
    "car = create_mesh(\"./data/meshes/Tesla/Tesla.obj\", device, normalise=True, position=[0, 0, 0], rotation=0, rot_axis=\"Y\", size_scale=2.2)\n",
    "\n",
    "# initialise renderer\n",
    "renderer = create_render(device, bin_size=None, faces=64000, light_loc=[10, 10, 10]) \n",
    "\n",
    "# render car and textureUV on test camera batch\n",
    "start_imgs, coords = render_batch_paste(car, renderer, test_cam, ontos=ontos)\n",
    "predicts, predicts_count = batch_predict(pred_model, start_imgs, coords, iou_thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278585b-c019-477f-9d06-fbdff9adb658",
   "metadata": {},
   "outputs": [],
   "source": [
    "see(predicts, figsize=(30, 30))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3627be8-9c4a-4e9a-9623-f4c3aa9afee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicts_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda8088-2fd1-4821-95a3-c2d82830689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy out patch from original texture\n",
    "texture_uv = get_texture_uv(car)\n",
    "patch = texture_uv.detach().clone()[:, 548:, :512, :]\n",
    "patch.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af61667d-7634-4bf1-97b9-c87541299690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify patch\n",
    "see(patch.clone().detach(), figsize=(5, 5))\n",
    "see_uv(car, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e18f6-65be-4e04-8e1f-f34900489e74",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f4228-a4df-4f25-92e9-55e8141af219",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = []\n",
    "classes_pred = []\n",
    "losses = []\n",
    "preds = []\n",
    "iters = []\n",
    "\n",
    "CLASSES = [2, 3, 4, 5, 6, 7]\n",
    "class_idxs = list(map(lambda x: x + 5, CLASSES))\n",
    "with open(\"./coco_classes.txt\", \"r\") as f:\n",
    "    classes = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a684211-b1cc-4495-b820-a1aec9cc08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "itera = 0\n",
    "loss = 8\n",
    "loop = trange(itera, position=0, leave=True);\n",
    "\n",
    "Adam = torch.optim.Adam([patch], lr=0.05)\n",
    "optimizer = Adam  # change this to select your preferred optimizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "while loss >= LOSS_THRESH:\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # reset optimizer and loss list\n",
    "    loss = torch.tensor(0.0, device=device)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # for Low Poly Car\n",
    "    copy = texture_uv.detach()\n",
    "    copy[0, 548:, :512, :] = patch\n",
    "    car.textures._maps_padded = copy\n",
    "    \n",
    "    scene = join(street, car)\n",
    "    \n",
    "    # render and predict on n training views\n",
    "    random_views = np.random.choice(train_total_views, MAX_VIEWS_PER_ITER, replace=False)\n",
    "    for view in random_views:\n",
    "        render_image = renderer(scene, cameras=train_cam[int(view)].get_camera())\n",
    "        image = preprocess(render_image, \"pred\")\n",
    "        pred = model(image) # pred is 2 x 1 x 25500 x 85\n",
    "        selection = torch.transpose(pred[0][0, :, 7 : 13], 0, 1) # make selection compatible with multiplication with obj [N * 6] -> [6 * N]\n",
    "        obj = pred[0][:, :, 4] # [1 * N]\n",
    "        loss += torch.max(selection * obj) # [N * 6] for conf for all anchor boxes. Maximum value has the index of the predicted class\n",
    "    \n",
    "    # avg loss over all views\n",
    "    loss /= MAX_VIEWS_PER_ITER\n",
    "    \n",
    "    writer.add_scalar(\"Loss\", loss, itera)\n",
    "    \n",
    "    if itera % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            current = render_one(scene, renderer, device, distance=5.0, elev=30, azim=30)\n",
    "            current = current.get_image()\n",
    "            current_img = current.squeeze().cpu()[..., :3].permute(2, 0, 1)\n",
    "            writer.add_image(\"Mesh\", torch.clamp(current_img, min=0.0, max=1.0), itera)\n",
    "            writer.add_image(\"Patch\", torch.clamp(patch.clone().detach().squeeze().permute(2, 0, 1), min=0.0, max=1.0), itera)\n",
    "            pred_standard = predict(pred_model, current, False)\n",
    "            returns.append(current_img.permute(1, 2, 0))\n",
    "            if len(pred_standard.xyxy[0]) == 0:\n",
    "                preds.append(None)\n",
    "                writer.add_scalar(\"Predicted Class\", -1, itera)\n",
    "                classes_pred.append(-1)\n",
    "            else:\n",
    "                writer.add_scalar(\"Predicted Class\", int(pred_standard.xyxy[0][0][5]), itera)\n",
    "                preds.append(classes[int(pred_standard.xyxy[0][0][5])])\n",
    "                classes_pred.append(int(pred_standard.xyxy[0][0][5]))\n",
    "            iters.append(itera)\n",
    "            \n",
    "    writer.flush()\n",
    "\n",
    "    # backprop\n",
    "    loop.set_description(f\"Conf loss = {loss:.6f}, iteration {itera}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(float(loss.clone().detach()))\n",
    "    itera += 1\n",
    "\n",
    "writer.close()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9dfdd2-3c4c-4cd1-b6ce-bc3bc5b6c549",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc9cbd-49f5-487e-97d2-1b7f2aa75685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run batch prediction to get final dicts of preds from test views\n",
    "ontos, d, e, a = load_data(dist=DIST_GRP, elev=ELEV_GRP)\n",
    "images, coords = render_batch_paste(car, renderer, test_cam, ontos=ontos)\n",
    "predicts, predict_count, adverses = batch_predict(pred_model, images, coords, iou_thres=0.5, adverse=True, adverse_classes=2)\n",
    "print(predict_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb8248-744e-4eaa-8f03-6a6e3890f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see(predicts, figsize=(30, 30))\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d8510-87af-4925-ad4d-06b160dccaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(predicts, path=f\"./images/{log_name}.png\", figsize=(50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee75ae-06bd-47ea-8f28-d0bb6eaad9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adverse_views = show_adverse(adverses, figsize=(10,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63575edd-0e5e-4991-8dc4-9ff47cffe42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render and predict on standard view\n",
    "final = render_one(scene, renderer, device, distance=5.0, elev=30, azim=30)\n",
    "_ = predict(pred_model, final, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f01c58-ebdc-47a3-85e0-3aa3697f664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = render_around(scene, renderer, device, batch_size=10, distance=5.0, elevMin=45, elevMax=45, azimMin=10, azimMax=350) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e11b3-5e15-4a11-91f7-f79613263245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see_uv(scene, figsize=(10, 10))\n",
    "# see(get_texture_uv(scene), path=\"./images/final_texture.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf4017-6ce2-411e-b3f0-8b02d196d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7334f64-44fd-4781-8b0b-04f42b63fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classes(classes_pred)\n",
    "print(classes_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
